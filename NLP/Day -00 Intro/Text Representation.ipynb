{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73bff3d-3cba-4440-91ff-6446834665e9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">Text Representation(Bag of Words, TF-IDF, N-Grams)</h2>\n",
    "\n",
    "--- \n",
    "\n",
    "### **üìå Bag of Words, TF-IDF, and N-Grams are important techniques for text data preprocessing in NLP.**\n",
    "\n",
    "### **üìù Text Representation**\n",
    "- üîπ **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ó ‡¶Ö‡¶´ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡¶∏ (Bag of Words - BoW)**\n",
    "- üîπ **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- üîπ **N-Grams (Uni-grams, Bi-grams, Tri-grams ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)**\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ Bag of Words - BoW**\n",
    "#### **‚û°Ô∏è Explainations:**\n",
    "**Bag of Words (BoW)** ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßá‡¶ú‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá **‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá (Sentence/Document) ‡¶™‡ßÉ‡¶•‡¶ï ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ Frequency ‡¶¨‡¶æ ‡¶ó‡¶£‡¶®‡¶æ‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßá‡¶ú‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§** ‡¶§‡¶¨‡ßá **‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ï‡ßç‡¶∞‡¶Æ ‡¶¨‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡¶ó‡¶§ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ‡¶®‡¶æ‡•§**\n",
    "\n",
    "### **üîπ How does it works?**\n",
    "1. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø **‡¶≠‡ßã‡¶ï‡¶æ‡¶¨‡ßÅ‡¶≤‡¶æ‡¶∞‡¶ø (Vocabulary)** ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§\n",
    "2. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá ‡¶ê ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ **‡¶ï‡¶§‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶∏‡ßá‡¶ü‡¶ø ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§**\n",
    "3. ‡¶è‡¶á ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá **‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (Vector) ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü**, ‡¶Ø‡¶æ **‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
    "\n",
    "### **üìå Example:**\n",
    "```text\n",
    "Text 1: \"I love NLP\"\n",
    "Text 2: \"I love Machine Learning\"\n",
    "Text 3: \"Machine Learning is amazing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6125e50-8aae-4524-9482-b6681319e244",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">‚≠ê NLP-‡¶§‡ßá Bag of Words, TF-IDF, ‡¶è‡¶¨‡¶Ç N-Grams Text data preprocessing-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ï‡ßå‡¶∂‡¶≤‡•§</h2>\n",
    "\n",
    "--- \n",
    "\n",
    "### **‚≠ê NLP-‡¶§‡ßá Text Representation ‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶∏‡¶π‡¶ú ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ**\n",
    "- üîπ **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ó ‡¶Ö‡¶´ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡¶∏ (Bag of Words - BoW)**\n",
    "- üîπ **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- üîπ **N-Grams (Uni-grams, Bi-grams, Tri-grams ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)**\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ Bag of Words - (BoW)**\n",
    "### **‚û°Ô∏è Explaination:**\n",
    "**Bag of Words (BoW)** ‡¶π‡¶≤‡ßã NLP-‡¶§‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßá‡¶ú‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá **‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá (Sentence/Document) ‡¶™‡ßÉ‡¶•‡¶ï ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ Frequency ‡¶¨‡¶æ ‡¶ó‡¶£‡¶®‡¶æ‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßá‡¶ú‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§** ‡¶§‡¶¨‡ßá **‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ï‡ßç‡¶∞‡¶Æ ‡¶¨‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡¶ó‡¶§ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ‡¶®‡¶æ‡•§**\n",
    "\n",
    "### **üîπ How does it work?**\n",
    "1. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø **‡¶≠‡ßã‡¶ï‡¶æ‡¶¨‡ßÅ‡¶≤‡¶æ‡¶∞‡¶ø (Vocabulary)** ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§\n",
    "2. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá ‡¶ê ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ **‡¶ï‡¶§‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶∏‡ßá‡¶ü‡¶ø ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§**\n",
    "3. ‡¶è‡¶á ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá **‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (Vector) ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü**, ‡¶Ø‡¶æ **‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç ‡¶Æ‡¶°‡ßá‡¶≤** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
    "\n",
    "### **üìå Disadvantages of Bag of Words:**\n",
    "‚ùå **‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ï‡ßç‡¶∞‡¶Æ ‡¶¨‡¶æ ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡ßã‡¶ù‡ßá ‡¶®‡¶æ‡•§**  \n",
    "‚ùå **‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶∏‡ßç‡¶™‡¶æ‡¶∞‡¶∏‡¶ø‡¶ü‡¶ø (Sparsity) ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü, ‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßú ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü‡•§**\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "### **‚û°Ô∏è Explaination:**\n",
    "TF-IDF ‡¶π‡¶≤‡ßã **‡¶è‡¶ï‡¶ü‡¶ø ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßá‡¶ú‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø**, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá:\n",
    "- **‡¶ï‡ßã‡¶®‡ßã ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶ï‡¶§‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡ßá‡¶õ‡ßá (TF)** ‡¶è‡¶¨‡¶Ç\n",
    "- **‡¶∏‡ßá‡¶ü‡¶ø ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶ó‡ßá‡¶õ‡ßá (IDF)** ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá **‡¶ì‡¶ú‡¶® (Weight) ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§**\n",
    "\n",
    "### **üîπ How does it work?**\n",
    "- **Term Frequency (TF):**  \n",
    "  - ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ / **‡¶Æ‡ßã‡¶ü ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡•§**\n",
    "- **Inverse Document Frequency (IDF):**  \n",
    "  - `log(‡¶Æ‡ßã‡¶ü ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ / ‡¶ê ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶Ø‡ßá ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡ßã‡¶§‡ßá ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ)`\n",
    "\n",
    "### **üìå Example:**\n",
    "üí° **‡¶Ø‡¶¶‡¶ø \"Machine\" ‡¶∂‡¶¨‡ßç‡¶¶‡¶ü‡¶ø ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶á ‡¶•‡¶æ‡¶ï‡ßá**, ‡¶§‡¶æ‡¶π‡¶≤‡ßá **IDF ‡¶ï‡¶Æ ‡¶π‡¶¨‡ßá**, ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶è‡¶ü‡¶ø ‡¶ï‡¶Æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡•§  \n",
    "üí° **‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Ø‡¶¶‡¶ø \"NLP\" ‡¶ï‡ßá‡¶¨‡¶≤ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶á ‡¶•‡¶æ‡¶ï‡ßá**, ‡¶§‡¶æ‡¶π‡¶≤‡ßá **IDF ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶¨‡ßá**, ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶è‡¶ü‡¶ø ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡•§\n",
    "\n",
    "### **‚≠ê TF-IDF ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:**\n",
    "‚úÖ ‡¶è‡¶ü‡¶ø **‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ì‡¶ú‡¶® ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¶‡ßá‡ßü**, ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶Æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã (‡¶Ø‡ßá‡¶Æ‡¶® `\"the\", \"is\", \"and\"`) ‡¶ï‡¶Æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨ ‡¶™‡¶æ‡ßü‡•§  \n",
    "‚úÖ **‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡¶∂‡¶®, ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®, ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶á‡¶û‡ßç‡¶ú‡¶ø‡¶® ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡¶ø‡¶Ç**-‡¶è ‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡•§  \n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **‡¶è‡¶ñ‡¶® Markdown ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶™‡ßú‡¶§‡ßá ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶∏‡¶π‡¶ú ‡¶π‡¶¨‡ßá!** üéØ  \n",
    "‡¶¨‡¶≤‡ßÅ‡¶®, ‡¶Ü‡¶∞ ‡¶ï‡ßã‡¶®‡ßã ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá? üòäüöÄ\n",
    "\n",
    "\n",
    "\n",
    "### 3Ô∏è‚É£ N-Grams (Uni-grams, Bi-grams, Tri-grams)\n",
    "‚û°Ô∏è ‡¶∏‡¶π‡¶ú ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:\n",
    "N-Grams ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶∞ ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶¨‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ß‡¶æ‡¶∞‡¶æ‡¶¨‡¶æ‡¶π‡¶ø‡¶ï ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø, ‡¶Ø‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ï‡ßç‡¶∞‡¶Æ (Order) ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡ßá‡•§\n",
    "\n",
    "### üîπ ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?\n",
    "- Uni-gram: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü‡•§\n",
    "- Bi-gram: ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶ï‡¶∞‡ßá ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü‡•§\n",
    "- Tri-gram: ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶ï‡¶∞‡ßá ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü‡•§\n",
    "    - Sentence: \"Natural Language Processing is amazing\"\n",
    "    - Uni-gram: [\"Natural\", \"Language\", \"Processing\", \"is\", \"amazing\"]\n",
    "    - Bi-gram: [\"Natural Language\", \"Language Processing\", \"Processing is\", \"is amazing\"]\n",
    "    - Tri-gram: [\"Natural Language Processing\", \"Language Processing is\", \"Processing is amazing\"]\n",
    "\n",
    "### ‚≠ê N-Grams-‡¶è‡¶∞ ‡¶â‡¶™‡¶ï‡¶æ‡¶∞‡¶ø‡¶§‡¶æ:\n",
    "- ‚úÖ ‡¶è‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ï‡ßç‡¶∞‡¶Æ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡ßá‡•§\n",
    "- ‚úÖ ‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡¶∂‡¶®, ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®, ‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶≤‡ßá‡¶∂‡¶®, ‡¶∏‡¶ø‡¶®‡¶ü‡ßç‡¶Ø‡¶æ‡¶ï‡¶ü‡¶ø‡¶ï ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶®‡¶æ‡¶≤‡¶æ‡¶á‡¶∏‡¶ø‡¶∏-‡¶è ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡•§\n",
    "- ‚úÖ Bi-gram ‡¶è‡¶¨‡¶Ç Tri-gram NLP-‡¶§‡ßá ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞‡ßÄ ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá‡•§\n",
    "\n",
    "#### üìå challenge:\n",
    "- ‚ùå ‡¶Ø‡¶¶‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá N-Gram-‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßú ‡¶π‡ßü‡ßá ‡¶Ø‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá (Dimensionality Issue)‡•§\n",
    "\n",
    "\n",
    "## ‚úÖ Applications in Real Life\n",
    "```text\n",
    "    ‚úî Spam Detection ‚Äì TF-IDF ‡¶ì BoW ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶á‡¶Æ‡ßá‡¶á‡¶≤ ‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§\n",
    "    ‚úî Search Engine Optimization (SEO) ‚Äì Google TF-IDF ‡¶ì N-Gram ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡¶ø‡¶Ç ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá‡•§\n",
    "    ‚úî Chatbot Development ‚Äì N-Grams ‡¶ì TF-IDF ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü‡ßá‡¶∞ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6f19-c34d-4205-a225-192ca307d02c",
   "metadata": {},
   "source": [
    "## Bag of Words using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7753c143-bf4e-4841-bc84-94838051def3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amazing  and  is  language  learning  love  machine  natural  nlp  \\\n",
      "0        0    0   0         1         0     1        0        1    0   \n",
      "1        1    0   1         1         0     0        0        1    0   \n",
      "2        0    1   0         0         1     1        1        0    1   \n",
      "\n",
      "   processing  \n",
      "0           1  \n",
      "1           1  \n",
      "2           0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Natural language processing is amazing!\",\n",
    "    \"I love machine learning and NLP.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words in vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert BoW matrix to array format\n",
    "bow_array = bow_matrix.toarray()\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "df = pd.DataFrame(bow_array, columns=feature_names)\n",
    "\n",
    "# Print the Bag of Words representation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607f3ea-b636-4e2f-81f9-c01fcab249e8",
   "metadata": {},
   "source": [
    "## TF-IDF using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2241e934-65e5-4ee7-980d-d945f219db6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amazing       and       is  language  learning      love   machine  \\\n",
      "0  0.00000  0.000000  0.00000  0.500000  0.000000  0.500000  0.000000   \n",
      "1  0.51742  0.000000  0.51742  0.393511  0.000000  0.000000  0.000000   \n",
      "2  0.00000  0.467351  0.00000  0.000000  0.467351  0.355432  0.467351   \n",
      "\n",
      "    natural       nlp  processing  \n",
      "0  0.500000  0.000000    0.500000  \n",
      "1  0.393511  0.000000    0.393511  \n",
      "2  0.000000  0.467351    0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Natural language processing is amazing!\",\n",
    "    \"I love machine learning and NLP.\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words in vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to array format\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "df = pd.DataFrame(tfidf_array, columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d63a3-b182-486d-95bf-19ac27a72f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3899db6-a99a-4b78-92d9-9ee28a531c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìù Bag of Words Representation:\n",
      "==================================================\n",
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n",
      "\n",
      "==================================================\n",
      "üìä TF-IDF Representation:\n",
      "==================================================\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n",
      "\n",
      "==================================================\n",
      "üìú N-gram Representation:\n",
      "==================================================\n",
      "\n",
      "‚≠ê **Sentence 1:** This is the first document.\n",
      "üîπ **Unigrams:** this, is, the, first, document\n",
      "üîπ **Bigrams:** this is, is the, the first, first document\n",
      "\n",
      "‚≠ê **Sentence 2:** This document is the second document.\n",
      "üîπ **Unigrams:** this, document, is, the, second, document\n",
      "üîπ **Bigrams:** this document, document is, is the, the second, second document\n",
      "\n",
      "‚≠ê **Sentence 3:** And this is the third one.\n",
      "üîπ **Unigrams:** and, this, is, the, third, one\n",
      "üîπ **Bigrams:** and this, this is, is the, the third, third one\n",
      "\n",
      "‚≠ê **Sentence 4:** Is this the first document?\n",
      "üîπ **Unigrams:** is, this, the, first, document\n",
      "üîπ **Bigrams:** is this, this the, the first, first document\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# NLTK data download functions\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Check and download necessary NLTK data.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "# text preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing: lowercase, remove punctuation, tokenize.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Bag-of-Words functions\n",
    "def bag_of_words(corpus):\n",
    "    \"\"\"Bag-of-Words representation.\"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(corpus)\n",
    "    return bow_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF functions\n",
    "def tf_idf(corpus):\n",
    "    \"\"\"TF-IDF representation.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# N-gram functions\n",
    "def n_grams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams.\"\"\"\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# main functions\n",
    "def main():\n",
    "    # NLTK download\n",
    "    download_nltk_resources()\n",
    "    \n",
    "    # sample text.\n",
    "    corpus = [\n",
    "        \"This is the first document.\",\n",
    "        \"This document is the second document.\",\n",
    "        \"And this is the third one.\",\n",
    "        \"Is this the first document?\"\n",
    "    ]\n",
    "\n",
    "    processed_corpus = [\" \".join(preprocess_text(text)) for text in corpus]  # preprocessing\n",
    "\n",
    "    # Bag of Words \n",
    "    bow_matrix, bow_features = bag_of_words(corpus)  # Use original corpus for BoW\n",
    "    df_bow = pd.DataFrame(bow_matrix.toarray(), columns=bow_features)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìù Bag of Words Representation:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_bow)\n",
    "    \n",
    "    # TF-IDF\n",
    "    tfidf_matrix, tfidf_features = tf_idf(corpus)  # Use original corpus for TF-IDF\n",
    "    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_features)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä TF-IDF Representation:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_tfidf)\n",
    "\n",
    "    # N-grams (Unigrams and Bigrams)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìú N-gram Representation:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, text in enumerate(corpus):\n",
    "        tokens = preprocess_text(text)\n",
    "        unigrams = n_grams(tokens, 1)\n",
    "        bigrams = n_grams(tokens, 2)\n",
    "\n",
    "        print(f\"\\n‚≠ê **Sentence {i+1}:** {text}\")\n",
    "        print(\"üîπ **Unigrams:**\", \", \".join([\" \".join(u) for u in unigrams]))\n",
    "        print(\"üîπ **Bigrams:**\", \", \".join([\" \".join(b) for b in bigrams]))\n",
    "\n",
    "# main functions\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97b24-1a91-44f2-b9e2-cb7296b6028b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
