{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73bff3d-3cba-4440-91ff-6446834665e9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">Text Representation(Bag of Words, TF-IDF, N-Grams)</h2>\n",
    "\n",
    "--- \n",
    "\n",
    "### **ЁЯУМ Bag of Words, TF-IDF, and N-Grams are important techniques for text data preprocessing in NLP.**\n",
    "\n",
    "### **ЁЯУЭ Text Representation**\n",
    "- ЁЯФ╣ **ржмрзНржпрж╛ржЧ ржЕржл ржУрзЯрж╛рж░рзНржбрж╕ (Bag of Words - BoW)**\n",
    "- ЁЯФ╣ **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- ЁЯФ╣ **N-Grams (Uni-grams, Bi-grams, Tri-grams ржЗрждрзНржпрж╛ржжрж┐)**\n",
    "\n",
    "---\n",
    "\n",
    "### **1я╕ПтГг Bag of Words - BoW**\n",
    "#### **тЮбя╕П Explainations:**\n",
    "**Bag of Words (BoW)** рж╣рж▓рзЛ ржПржХржЯрж┐ рж╕рж╛ржзрж╛рж░ржг ржЯрзЗржХрзНрж╕ржЯ рж░рж┐ржкрзНрж░рзЗржЬрзЗржирзНржЯрзЗрж╢ржи ржЯрзЗржХржирж┐ржХ, ржпрзЗржЦрж╛ржирзЗ **ржПржХржЯрж┐ ржЯрзЗржХрзНрж╕ржЯржХрзЗ (Sentence/Document) ржкрзГржержХ рж╢ржмрзНржжрзЗрж░ Frequency ржмрж╛ ржЧржгржирж╛рж░ ржнрж┐рждрзНрждрж┐рждрзЗ рж░рж┐ржкрзНрж░рзЗржЬрзЗржирзНржЯ ржХрж░рж╛ рж╣рзЯред** рждржмрзЗ **рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржХрзНрж░ржо ржмрж╛ ржЕрж░рзНржержЧржд рж╕ржорзНржкрж░рзНржХ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рж╛ рж╣рзЯ ржирж╛ред**\n",
    "\n",
    "### **ЁЯФ╣ How does it works?**\n",
    "1. ржкрзНрж░рждрж┐ржЯрж┐ ржЗржЙржирж┐ржХ рж╢ржмрзНржжрзЗрж░ ржПржХржЯрж┐ **ржнрзЛржХрж╛ржмрзБрж▓рж╛рж░рж┐ (Vocabulary)** рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯред\n",
    "2. ржкрзНрж░рждрж┐ржЯрж┐ ржбржХрзБржорзЗржирзНржЯрзЗ ржР рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ **ржХрждржмрж╛рж░ ржмрзНржпржмрж╣рзГржд рж╣рзЯрзЗржЫрзЗ рж╕рзЗржЯрж┐ ржЧржгржирж╛ ржХрж░рж╛ рж╣рзЯред**\n",
    "3. ржПржЗ рждржерзНржп ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ **ржПржХржЯрж┐ ржнрзЗржХрзНржЯрж░ (Vector) рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯ**, ржпрж╛ **ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ ржоржбрзЗрж▓** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ ржкрж╛рж░рзЗред\n",
    "\n",
    "### **ЁЯУМ Example:**\n",
    "```text\n",
    "Text 1: \"I love NLP\"\n",
    "Text 2: \"I love Machine Learning\"\n",
    "Text 3: \"Machine Learning is amazing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6125e50-8aae-4524-9482-b6681319e244",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">тнР NLP-рждрзЗ Bag of Words, TF-IDF, ржПржмржВ N-Grams Text data preprocessing-ржПрж░ ржЬржирзНржп ржЕржирзНржпрждржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржХрзМрж╢рж▓ред</h2>\n",
    "\n",
    "--- \n",
    "\n",
    "### **тнР NLP-рждрзЗ Text Representation ржмрзЛржЭрж╛рж░ рж╕рж╣ржЬ ржмрзНржпрж╛ржЦрзНржпрж╛**\n",
    "- ЁЯФ╣ **ржмрзНржпрж╛ржЧ ржЕржл ржУрзЯрж╛рж░рзНржбрж╕ (Bag of Words - BoW)**\n",
    "- ЁЯФ╣ **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- ЁЯФ╣ **N-Grams (Uni-grams, Bi-grams, Tri-grams ржЗрждрзНржпрж╛ржжрж┐)**\n",
    "\n",
    "---\n",
    "\n",
    "### **1я╕ПтГг Bag of Words - (BoW)**\n",
    "### **тЮбя╕П Explaination:**\n",
    "**Bag of Words (BoW)** рж╣рж▓рзЛ NLP-рждрзЗ ржПржХржЯрж┐ рж╕рж╛ржзрж╛рж░ржг ржЯрзЗржХрзНрж╕ржЯ рж░рж┐ржкрзНрж░рзЗржЬрзЗржирзНржЯрзЗрж╢ржи ржЯрзЗржХржирж┐ржХ, ржпрзЗржЦрж╛ржирзЗ **ржПржХржЯрж┐ ржЯрзЗржХрзНрж╕ржЯржХрзЗ (Sentence/Document) ржкрзГржержХ рж╢ржмрзНржжрзЗрж░ Frequency ржмрж╛ ржЧржгржирж╛рж░ ржнрж┐рждрзНрждрж┐рждрзЗ рж░рж┐ржкрзНрж░рзЗржЬрзЗржирзНржЯ ржХрж░рж╛ рж╣рзЯред** рждржмрзЗ **рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржХрзНрж░ржо ржмрж╛ ржЕрж░рзНржержЧржд рж╕ржорзНржкрж░рзНржХ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рж╛ рж╣рзЯ ржирж╛ред**\n",
    "\n",
    "### **ЁЯФ╣ How does it work?**\n",
    "1. ржкрзНрж░рждрж┐ржЯрж┐ ржЗржЙржирж┐ржХ рж╢ржмрзНржжрзЗрж░ ржПржХржЯрж┐ **ржнрзЛржХрж╛ржмрзБрж▓рж╛рж░рж┐ (Vocabulary)** рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯред\n",
    "2. ржкрзНрж░рждрж┐ржЯрж┐ ржбржХрзБржорзЗржирзНржЯрзЗ ржР рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ **ржХрждржмрж╛рж░ ржмрзНржпржмрж╣рзГржд рж╣рзЯрзЗржЫрзЗ рж╕рзЗржЯрж┐ ржЧржгржирж╛ ржХрж░рж╛ рж╣рзЯред**\n",
    "3. ржПржЗ рждржерзНржп ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ **ржПржХржЯрж┐ ржнрзЗржХрзНржЯрж░ (Vector) рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯ**, ржпрж╛ **ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ ржоржбрзЗрж▓** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ ржкрж╛рж░рзЗред\n",
    "\n",
    "### **ЁЯУМ Disadvantages of Bag of Words:**\n",
    "тЭМ **рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржХрзНрж░ржо ржмрж╛ ржХржиржЯрзЗржХрзНрж╕ржЯ ржмрзЛржЭрзЗ ржирж╛ред**  \n",
    "тЭМ **ржЦрзБржм ржмрзЗрж╢рж┐ рж╢ржмрзНржж ржерж╛ржХрж▓рзЗ рж╕рзНржкрж╛рж░рж╕рж┐ржЯрж┐ (Sparsity) рждрзИрж░рж┐ рж╣рзЯ, ржЕрж░рзНржерж╛рзО ржорзНржпрж╛ржЯрзНрж░рж┐ржХрзНрж╕ржЯрж┐ ржЕржирзЗржХ ржмрзЬ рж╣рзЯрзЗ ржпрж╛рзЯред**\n",
    "\n",
    "---\n",
    "\n",
    "### **2я╕ПтГг TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "### **тЮбя╕П Explaination:**\n",
    "TF-IDF рж╣рж▓рзЛ **ржПржХржЯрж┐ ржЙржирзНржиржд ржЯрзЗржХрзНрж╕ржЯ рж░рж┐ржкрзНрж░рзЗржЬрзЗржирзНржЯрзЗрж╢ржи ржкржжрзНржзрждрж┐**, ржпрзЗржЦрж╛ржирзЗ:\n",
    "- **ржХрзЛржирзЛ рж╢ржмрзНржж ржХрждржмрж╛рж░ ржмрзНржпржмрж╣рзГржд рж╣рзЯрзЗржЫрзЗ (TF)** ржПржмржВ\n",
    "- **рж╕рзЗржЯрж┐ ржХрждржЧрзБрж▓рзЛ ржбржХрзБржорзЗржирзНржЯрзЗ ржкрж╛ржУрзЯрж╛ ржЧрзЗржЫрзЗ (IDF)** ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ **ржУржЬржи (Weight) ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж╛ рж╣рзЯред**\n",
    "\n",
    "### **ЁЯФ╣ How does it work?**\n",
    "- **Term Frequency (TF):**  \n",
    "  - ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржПржХржЯрж┐ рж╢ржмрзНржжрзЗрж░ рж╕ржВржЦрзНржпрж╛ / **ржорзЛржЯ рж╢ржмрзНржжрзЗрж░ рж╕ржВржЦрзНржпрж╛ред**\n",
    "- **Inverse Document Frequency (IDF):**  \n",
    "  - `log(ржорзЛржЯ ржбржХрзБржорзЗржирзНржЯ рж╕ржВржЦрзНржпрж╛ / ржР рж╢ржмрзНржж ржпрзЗ ржбржХрзБржорзЗржирзНржЯржЧрзБрж▓рзЛрждрзЗ ржЖржЫрзЗ рждрж╛рж░ рж╕ржВржЦрзНржпрж╛)`\n",
    "\n",
    "### **ЁЯУМ Example:**\n",
    "ЁЯТб **ржпржжрж┐ \"Machine\" рж╢ржмрзНржжржЯрж┐ ржжрзБржЗржЯрж┐ ржбржХрзБржорзЗржирзНржЯрзЗржЗ ржерж╛ржХрзЗ**, рждрж╛рж╣рж▓рзЗ **IDF ржХржо рж╣ржмрзЗ**, ржХрж╛рж░ржг ржПржЯрж┐ ржХржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржгред  \n",
    "ЁЯТб **ржХрж┐ржирзНрждрзБ ржпржжрж┐ \"NLP\" ржХрзЗржмрж▓ ржПржХржЯрж┐ ржбржХрзБржорзЗржирзНржЯрзЗржЗ ржерж╛ржХрзЗ**, рждрж╛рж╣рж▓рзЗ **IDF ржмрзЗрж╢рж┐ рж╣ржмрзЗ**, ржХрж╛рж░ржг ржПржЯрж┐ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржгред\n",
    "\n",
    "### **тнР TF-IDF ржмрзНржпржмрж╣рж╛рж░рзЗрж░ рж╕рзБржмрж┐ржзрж╛:**\n",
    "тЬЕ ржПржЯрж┐ **ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржУржЬржи ржмрзЗрж╢рж┐ ржжрзЗрзЯ**, ржПржмржВ ржХржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╢ржмрзНржжржЧрзБрж▓рзЛ (ржпрзЗржоржи `\"the\", \"is\", \"and\"`) ржХржо ржЧрзБрж░рзБрждрзНржм ржкрж╛рзЯред  \n",
    "тЬЕ **рж╕рзНржкрзНржпрж╛ржо ржбрж┐ржЯрзЗржХрж╢ржи, ржбржХрзБржорзЗржирзНржЯ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж╛ржЗржЬрзЗрж╢ржи, ржПржмржВ рж╕рж╛рж░рзНржЪ ржЗржЮрзНржЬрж┐ржи рж░тАНрзНржпрж╛ржВржХрж┐ржВ**-ржП ржмрзНржпрж╛ржкржХржнрж╛ржмрзЗ ржмрзНржпржмрж╣рзГржд рж╣рзЯред  \n",
    "\n",
    "---\n",
    "\n",
    "тЬЕ **ржПржЦржи Markdown ржлрж░ржорзНржпрж╛ржЯрзЗ ржкрзЬрждрзЗ ржПржХржжржо рж╕рж╣ржЬ рж╣ржмрзЗ!** ЁЯОп  \n",
    "ржмрж▓рзБржи, ржЖрж░ ржХрзЛржирзЛ ржкрж░рж┐ржмрж░рзНрждржи рж▓рж╛ржЧржмрзЗ? ЁЯШКЁЯЪА\n",
    "\n",
    "\n",
    "\n",
    "### 3я╕ПтГг N-Grams (Uni-grams, Bi-grams, Tri-grams)\n",
    "тЮбя╕П рж╕рж╣ржЬ ржмрзНржпрж╛ржЦрзНржпрж╛:\n",
    "N-Grams рж╣рж▓рзЛ ржПржХржЯрж┐ ржЯрзЗржХрзНрж╕ржЯрзЗрж░ ржЯрзЛржХрзЗржи ржмрж╛ рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХ ржЧрзНрж░рзБржк рждрзИрж░рж┐ ржХрж░рж╛рж░ ржПржХржЯрж┐ ржкржжрзНржзрждрж┐, ржпрж╛ рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржХрзНрж░ржо (Order) рж╕ржВрж░ржХрзНрж╖ржг ржХрж░рзЗред\n",
    "\n",
    "### ЁЯФ╣ ржХрж┐ржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ?\n",
    "- Uni-gram: ржкрзНрж░рждрж┐ржЯрж┐ рж╢ржмрзНржж ржЖрж▓рж╛ржжрж╛ ржЖрж▓рж╛ржжрж╛ ржирзЗржУрзЯрж╛ рж╣рзЯред\n",
    "- Bi-gram: ржПржХрж╕рж╛ржерзЗ ржжрзБржЗржЯрж┐ ржХрж░рзЗ рж╢ржмрзНржж ржирзЗржУрзЯрж╛ рж╣рзЯред\n",
    "- Tri-gram: ржПржХрж╕рж╛ржерзЗ рждрж┐ржиржЯрж┐ ржХрж░рзЗ рж╢ржмрзНржж ржирзЗржУрзЯрж╛ рж╣рзЯред\n",
    "    - Sentence: \"Natural Language Processing is amazing\"\n",
    "    - Uni-gram: [\"Natural\", \"Language\", \"Processing\", \"is\", \"amazing\"]\n",
    "    - Bi-gram: [\"Natural Language\", \"Language Processing\", \"Processing is\", \"is amazing\"]\n",
    "    - Tri-gram: [\"Natural Language Processing\", \"Language Processing is\", \"Processing is amazing\"]\n",
    "\n",
    "### тнР N-Grams-ржПрж░ ржЙржкржХрж╛рж░рж┐рждрж╛:\n",
    "- тЬЕ ржПржЯрж┐ рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржХрзНрж░ржо рж╕ржВрж░ржХрзНрж╖ржг ржХрж░рзЗред\n",
    "- тЬЕ рж╕рзНржкрзНржпрж╛ржо ржбрж┐ржЯрзЗржХрж╢ржи, ржЯрзЗржХрзНрж╕ржЯ ржЬрзЗржирж╛рж░рзЗрж╢ржи, ржорзЗрж╢рж┐ржи ржЯрзНрж░рж╛ржирзНрж╕рж▓рзЗрж╢ржи, рж╕рж┐ржиржЯрзНржпрж╛ржХржЯрж┐ржХ ржЕрзНржпрж╛ржирж╛рж▓рж╛ржЗрж╕рж┐рж╕-ржП ржмрзНржпржмрж╣рзГржд рж╣рзЯред\n",
    "- тЬЕ Bi-gram ржПржмржВ Tri-gram NLP-рждрзЗ ржЦрзБржмржЗ ржХрж╛рж░рзНржпржХрж░рзА ржХрж╛рж░ржг ржПржЧрзБрж▓рзЛ ржХржиржЯрзЗржХрзНрж╕ржЯ ржмрзБржЭрждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗред\n",
    "\n",
    "#### ЁЯУМ challenge:\n",
    "- тЭМ ржпржжрж┐ ржЯрзЗржХрзНрж╕ржЯрзЗ ржЕржирзЗржХ рж╢ржмрзНржж ржерж╛ржХрзЗ, рждрж╛рж╣рж▓рзЗ N-Gram-ржнрж┐рждрзНрждрж┐ржХ ржнрзЗржХрзНржЯрж░ ржмрзЬ рж╣рзЯрзЗ ржпрзЗрждрзЗ ржкрж╛рж░рзЗ (Dimensionality Issue)ред\n",
    "\n",
    "\n",
    "## тЬЕ Applications in Real Life\n",
    "```text\n",
    "    тЬФ Spam Detection тАУ TF-IDF ржУ BoW ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЗржорзЗржЗрж▓ рж╕рзНржкрзНржпрж╛ржо ржбрж┐ржЯрзЗржХрж╢ржи ржХрж░рж╛ рж╣рзЯред\n",
    "    тЬФ Search Engine Optimization (SEO) тАУ Google TF-IDF ржУ N-Gram ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ рж░тАНрзНржпрж╛ржВржХрж┐ржВ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред\n",
    "    тЬФ Chatbot Development тАУ N-Grams ржУ TF-IDF ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЪрзНржпрж╛ржЯржмржЯрзЗрж░ ржЯрзЗржХрзНрж╕ржЯ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ ржЙржирзНржиржд ржХрж░рж╛ рж╣рзЯред"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6f19-c34d-4205-a225-192ca307d02c",
   "metadata": {},
   "source": [
    "## Bag of Words using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7753c143-bf4e-4841-bc84-94838051def3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amazing  and  is  language  learning  love  machine  natural  nlp  \\\n",
      "0        0    0   0         1         0     1        0        1    0   \n",
      "1        1    0   1         1         0     0        0        1    0   \n",
      "2        0    1   0         0         1     1        1        0    1   \n",
      "\n",
      "   processing  \n",
      "0           1  \n",
      "1           1  \n",
      "2           0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Natural language processing is amazing!\",\n",
    "    \"I love machine learning and NLP.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words in vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert BoW matrix to array format\n",
    "bow_array = bow_matrix.toarray()\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "df = pd.DataFrame(bow_array, columns=feature_names)\n",
    "\n",
    "# Print the Bag of Words representation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607f3ea-b636-4e2f-81f9-c01fcab249e8",
   "metadata": {},
   "source": [
    "## TF-IDF using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2241e934-65e5-4ee7-980d-d945f219db6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amazing       and       is  language  learning      love   machine  \\\n",
      "0  0.00000  0.000000  0.00000  0.500000  0.000000  0.500000  0.000000   \n",
      "1  0.51742  0.000000  0.51742  0.393511  0.000000  0.000000  0.000000   \n",
      "2  0.00000  0.467351  0.00000  0.000000  0.467351  0.355432  0.467351   \n",
      "\n",
      "    natural       nlp  processing  \n",
      "0  0.500000  0.000000    0.500000  \n",
      "1  0.393511  0.000000    0.393511  \n",
      "2  0.000000  0.467351    0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Natural language processing is amazing!\",\n",
    "    \"I love machine learning and NLP.\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words in vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to array format\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "df = pd.DataFrame(tfidf_array, columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d63a3-b182-486d-95bf-19ac27a72f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3899db6-a99a-4b78-92d9-9ee28a531c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ЁЯУЭ Bag of Words Representation:\n",
      "==================================================\n",
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n",
      "\n",
      "==================================================\n",
      "ЁЯУК TF-IDF Representation:\n",
      "==================================================\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n",
      "\n",
      "==================================================\n",
      "ЁЯУЬ N-gram Representation:\n",
      "==================================================\n",
      "\n",
      "тнР **Sentence 1:** This is the first document.\n",
      "ЁЯФ╣ **Unigrams:** this, is, the, first, document\n",
      "ЁЯФ╣ **Bigrams:** this is, is the, the first, first document\n",
      "\n",
      "тнР **Sentence 2:** This document is the second document.\n",
      "ЁЯФ╣ **Unigrams:** this, document, is, the, second, document\n",
      "ЁЯФ╣ **Bigrams:** this document, document is, is the, the second, second document\n",
      "\n",
      "тнР **Sentence 3:** And this is the third one.\n",
      "ЁЯФ╣ **Unigrams:** and, this, is, the, third, one\n",
      "ЁЯФ╣ **Bigrams:** and this, this is, is the, the third, third one\n",
      "\n",
      "тнР **Sentence 4:** Is this the first document?\n",
      "ЁЯФ╣ **Unigrams:** is, this, the, first, document\n",
      "ЁЯФ╣ **Bigrams:** is this, this the, the first, first document\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# NLTK data download functions\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Check and download necessary NLTK data.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "# text preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing: lowercase, remove punctuation, tokenize.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Bag-of-Words functions\n",
    "def bag_of_words(corpus):\n",
    "    \"\"\"Bag-of-Words representation.\"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(corpus)\n",
    "    return bow_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF functions\n",
    "def tf_idf(corpus):\n",
    "    \"\"\"TF-IDF representation.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# N-gram functions\n",
    "def n_grams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams.\"\"\"\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# main functions\n",
    "def main():\n",
    "    # NLTK download\n",
    "    download_nltk_resources()\n",
    "    \n",
    "    # sample text.\n",
    "    corpus = [\n",
    "        \"This is the first document.\",\n",
    "        \"This document is the second document.\",\n",
    "        \"And this is the third one.\",\n",
    "        \"Is this the first document?\"\n",
    "    ]\n",
    "\n",
    "    processed_corpus = [\" \".join(preprocess_text(text)) for text in corpus]  # preprocessing\n",
    "\n",
    "    # Bag of Words \n",
    "    bow_matrix, bow_features = bag_of_words(corpus)  # Use original corpus for BoW\n",
    "    df_bow = pd.DataFrame(bow_matrix.toarray(), columns=bow_features)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ЁЯУЭ Bag of Words Representation:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_bow)\n",
    "    \n",
    "    # TF-IDF\n",
    "    tfidf_matrix, tfidf_features = tf_idf(corpus)  # Use original corpus for TF-IDF\n",
    "    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_features)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ЁЯУК TF-IDF Representation:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_tfidf)\n",
    "\n",
    "    # N-grams (Unigrams and Bigrams)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ЁЯУЬ N-gram Representation:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, text in enumerate(corpus):\n",
    "        tokens = preprocess_text(text)\n",
    "        unigrams = n_grams(tokens, 1)\n",
    "        bigrams = n_grams(tokens, 2)\n",
    "\n",
    "        print(f\"\\nтнР **Sentence {i+1}:** {text}\")\n",
    "        print(\"ЁЯФ╣ **Unigrams:**\", \", \".join([\" \".join(u) for u in unigrams]))\n",
    "        print(\"ЁЯФ╣ **Bigrams:**\", \", \".join([\" \".join(b) for b in bigrams]))\n",
    "\n",
    "# main functions\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97b24-1a91-44f2-b9e2-cb7296b6028b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
