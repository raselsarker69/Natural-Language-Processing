{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1644c2c5-b9c0-4788-999a-1128ceb9916a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">Correct Order for Text Preprocessing</h2>\n",
    "\n",
    "- **Text extraction → Noise Removal → Tokenization → Lemmatization/Stemming → Normalization.**\n",
    "---\n",
    "#### 1. **Text Extraction (PDF to Text)**\n",
    "- Extract text from the PDF using OCR or other tools (like `pytesseract`).\n",
    "\n",
    "#### 2. **Noise Removal (Optional at This Stage)**\n",
    "- Remove unnecessary characters (e.g., special symbols, extra spaces, or unwanted HTML tags) **before tokenization** for a cleaner output.\n",
    "- **Why here?** It ensures that tokenization doesn't create unnecessary tokens from noisy characters.\n",
    "\n",
    "#### 3. **Tokenization**\n",
    "- Break the cleaned text into smaller units (e.g., words or sentences).\n",
    "- **Why here?** Lemmatization/Stemming operates at the word level, so text needs to be tokenized first.\n",
    "\n",
    "#### 4. **Lemmatization or Stemming (Choose One)**\n",
    "- Reduce tokens to their root or base forms:\n",
    "  - **Lemmatization**: Produces meaningful root words using linguistic rules.\n",
    "  - **Stemming**: Uses simpler rules to strip suffixes (can result in non-meaningful words).\n",
    "\n",
    "#### 5. **Normalization**\n",
    "- Convert the text to a consistent format (e.g., lowercase, fixing spelling variations).\n",
    "- **Why here?** Ensures consistency for downstream tasks like text classification or matching.\n",
    "\n",
    "---\n",
    "\n",
    "### **Text Preprocessing Correct Order**\n",
    "1. **Text Extraction (PDF to Text)**\n",
    "2. **Noise Removal** (Remove unwanted characters before tokenization)\n",
    "3. **Tokenization** (Break text into tokens)\n",
    "4. **Lemmatization/Stemming** (Reduce words to their base forms)\n",
    "5. **Normalization** (Lowercasing, fixing spelling inconsistencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367ed78-3513-4400-b028-2caf55b8f155",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">ETL(ETL (Extract, Transform, Load)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8521a-aa85-470e-970e-76a54b5206d3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">System Architecture for Multi-Source Data Processing</h2>\n",
    "    \n",
    "---\n",
    "    \n",
    "\n",
    "## **1. Data Sources**\n",
    "- **SQL File:** Database storage and retrieval.\n",
    "- **API:** Accessing external or internal services.\n",
    "- **Web Scraping:** Extracting data from websites.\n",
    "- **Live Streaming:** Data from sensors, drones, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Loaders**\n",
    "- **Pandas:** Suitable for datasets up to **5 GB**.\n",
    "- **Polars:** Ideal for datasets up to **10 GB**.\n",
    "- **Apache Spark:** Handles large-scale datasets in **Petabytes (PB)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Understanding Source-Specific Functionality**\n",
    "In systems with multiple data sources, it is crucial to identify:\n",
    "- **Which source requires which function?**\n",
    "This can be achieved using a well-designed source routing mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Source Routing Mechanisms**\n",
    "\n",
    "### **4.1 Meta (Dictionary) Based Source Routing**\n",
    "- Use metadata to dynamically route sources.\n",
    "- Example: Efficient handling of file-based data sources.\n",
    "\n",
    "### **4.2 Event-Driven Architecture**\n",
    "- Suitable for processing live-streaming data.\n",
    "- Example: Sensor or drone data.\n",
    "\n",
    "### **4.3 Microservice-Based API Gateway**\n",
    "- Use APIs for seamless data management.\n",
    "- Example: Handling data in **JSON** format.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "This architecture ensures efficient data handling, source-specific routing, and optimized use of system resources based on the size and type of the dataset. If needed, further customizations can be made to enhance system performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c0094-72e2-43f1-9d99-e82ade2ef552",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">System Architecture for Data Processing</h2>\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Source Routing Mechanisms**\n",
    "1. **Meta (Dictionary) Based Source Routing**  \n",
    "   - File-based source routing mechanism for efficient processing.\n",
    "\n",
    "2. **Event-Driven Architecture**  \n",
    "   - Handles live streaming data, such as data from sensors or drones.\n",
    "\n",
    "3. **Microservice: API Gateway → JSON**  \n",
    "   - Processes and routes data via APIs in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Services**\n",
    "1. **Recommendation Engine:** Implemented in Python.  \n",
    "2. **Face Recognition:** Developed using Python.  \n",
    "3. **Facebook Post Processing:** Built with PHP.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Transformation**\n",
    "- Types of Processing:\n",
    "  - **Text Processing**  \n",
    "  - **Voice Processing**  \n",
    "  - **Image Processing**\n",
    "\n",
    "### Workflow:\n",
    "- **Input → Transformation → ML/DL/LLM Model**\n",
    "  - Includes steps like:\n",
    "    - Text Cleaning\n",
    "    - Text Structuring\n",
    "    - Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Loading**\n",
    "- **Processed Data:** \n",
    "  - [Vectorized Data / Pixel Data: Numeric Format]  \n",
    "  - Data sent to the destination.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Destination**\n",
    "- **Storage Options:**\n",
    "  - Cloud Server\n",
    "  - DBMS\n",
    "  - CSV\n",
    "  - Vector Database\n",
    "  - Warehouse (Cleaned Dataset)\n",
    "\n",
    "---\n",
    "\n",
    "### **Note:**\n",
    "This architecture optimizes processing by leveraging the appropriate source routing mechanism and storage solution for different types of data and use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f659175-6d2a-46da-8c81-846239c7c596",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color: #f9f9fc; color: #333366; border-radius: 12px; margin: 20px auto; padding: 20px; border: 2px solid #ff4c4c; max-width: 1000px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "  <h2 style=\"text-align: center; color: #333366;\">ETL (Extract, Transform, Load) and NLP</h2>\n",
    "\n",
    "\n",
    "ETL stands for **Extract**, **Transform** and **Load** – এটি একটি Data processing steps যা সাধারণত **Natural Language Processing (NLP)** সহ বিভিন্ন data engineering and data projects a ব্যবহৃত হয়।\n",
    "\n",
    "---\n",
    "\n",
    "## **ETL-এর ধাপসমূহ NLP-তে**\n",
    "\n",
    "### **১. Extract করা**\n",
    "ডেটা সংগ্রহ করার প্রক্রিয়া। NLP-তে ডেটা সংগ্রহ করা হয় বিভিন্ন উৎস থেকে:\n",
    "- **Website:** (ওয়েব স্ক্র্যাপিং)\n",
    "- **Database:** (SQL, NoSQL)\n",
    "- **API:** (Twitter API, OpenAI API)\n",
    "- **Text Documents:** (PDF, CSV, Word)\n",
    "\n",
    "#### **Example:**  \n",
    "কোনো সংবাদপত্রের ওয়েবসাইট থেকে খবর সংগ্রহ করা।\n",
    "\n",
    "---\n",
    "\n",
    "### **২. Transform করা**\n",
    "এই ধাপে ডেটা **Cleaning** ও **Processing** করা হয় যাতে এটি মডেলের জন্য ব্যবহারযোগ্য হয়।\n",
    "\n",
    "#### **NLP-তে Transform ধাপের কাজগুলো:**\n",
    "- **Text Cleaning:** অনাবশ্যক চিহ্ন, HTML ট্যাগ, স্টপওয়ার্ড বাদ দেওয়া।\n",
    "- **Tokenization:** বাক্যকে শব্দে ভাগ করা।\n",
    "- **Stemming & Lemmatization:** শব্দের মূল রূপ নির্ধারণ করা।\n",
    "- **POS Tagging:** Part of Speech নির্ধারণ করা।\n",
    "- **Vectorization:** TF-IDF, Word2Vec, BERT ইত্যাদির মাধ্যমে টেক্সটকে সংখ্যায় রূপান্তর করা।\n",
    "\n",
    "#### **Example:**  \n",
    "\"আমি ভারতে গিয়েছিলাম।\"  \n",
    "→ **Tokenization:** [\"আমি\", \"ভারত\", \"যাওয়া\"]  \n",
    "→ **Lemmatization:** [\"আমি\", \"ভারত\", \"যাই\"]\n",
    "\n",
    "---\n",
    "\n",
    "### **৩. Load করা**\n",
    "প্রসেসকৃত ডেটা সংরক্ষণ করা হয় **Database** বা **মডেল ট্রেনিংয়ের জন্য।**\n",
    "\n",
    "#### **NLP-তে এটি হতে পারে:**\n",
    "- ML/DL মডেল ট্রেনিংয়ের জন্য ডেটা লোড করা।\n",
    "- Big Data Storage (Hadoop, Spark, SQL)।\n",
    "- ব্যবহারকারীর জন্য API তৈরি করা।\n",
    "\n",
    "#### **Example:**  \n",
    "টুইট বিশ্লেষণ করার পর সংক্ষেপিত তথ্য ডাটাবেজে সংরক্ষণ করা।\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is ETL important in NLP??**\n",
    "✅ **অনিয়ন্ত্রিত ডেটা সংগ্রহ ও বিশ্লেষণ সহজ করে।**  \n",
    "✅ **NLP মডেলের জন্য পরিষ্কার ডেটা প্রস্তুত করে।**  \n",
    "✅ **স্বয়ংক্রিয় টেক্সট প্রক্রিয়াকরণ এবং AI ভিত্তিক বিশ্লেষণে সহায়ক।**\n",
    "\n",
    "---\n",
    "\n",
    "## **Uses of ETL in NLP**\n",
    "- **News Classification.**\n",
    "- **Spam Filtering.**\n",
    "- **Language Translation.**\n",
    "- **Chatbot.**\n",
    "- **Sentiment Analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b2892-dfef-426f-921e-ca3a75801c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a051c12-a8c5-4a6e-8b34-f8d613699543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52325bd3-0a01-454f-9620-1be1c36fc441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d3b24-5736-4d6c-8422-c3e54d587bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8311b-7f3d-46b1-bee9-d95fb2513816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
