{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cf1293-666c-4c2c-bf14-7be78c0dcaf4",
   "metadata": {},
   "source": [
    "## There are three main approaches in Natural Language Processing (NLP):\n",
    "- Heuristic-based approach (Rule-based methods).\n",
    "- Machine Learning approach.\n",
    "- Deep Learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f442bf0-1cbc-45c3-8c15-5cf0f62be885",
   "metadata": {},
   "source": [
    "# Machine Learning Approach\n",
    "#### The Big Advantage\n",
    "#### ML Workflow\n",
    "#### Algorithms Used\n",
    "- **Naive Bayes**\n",
    "- **Logistic Regression**\n",
    "- **SVM**\n",
    "- **LDA**\n",
    "- **Hidden Markov Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099e66f-815b-44d7-8f6e-4b0e8c87d4ae",
   "metadata": {},
   "source": [
    "## üåü Deep Learning Approach\n",
    "#### üöÄ The Big Advantage\n",
    "#### üèóÔ∏è Architectures Used\n",
    "- üîÑ **RNN** (Recurrent Neural Network)\n",
    "- ‚è≥ **LSTM** (Long Short-Term Memory)\n",
    "- üîó **GRU/CNN** (Gated Recurrent Unit / Convolutional Neural Network)\n",
    "- ü§ñ **Transformers** (Attention-based Models)\n",
    "- üåÄ **Autoencoders** (Unsupervised Feature Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626a7cb-3c4b-4868-ae69-7a69836134d8",
   "metadata": {},
   "source": [
    "# üöÄ Challenges in NLP (Natural Language Processing)\n",
    "\n",
    "### 1Ô∏è‚É£ Ambiguity  \n",
    "   - **Lexical Ambiguity**: Words with multiple meanings (e.g., *\"bank\"*: riverbank vs. financial institution).  \n",
    "   - **Syntactic Ambiguity**: Multiple possible sentence structures (*\"I saw the boy on the beach with my binoculars.\"*).  \n",
    "   - **Semantic Ambiguity**: Confusing meaning (*\"Flying planes can be dangerous.\"* - Are planes dangerous, or is flying them dangerous?).\n",
    "\n",
    "### 2Ô∏è‚É£ Contextual Understanding  \n",
    "   - Words depend on **context** (e.g., *\"I **ran** to the store because we **ran** out of milk.\"*).  \n",
    "   - Requires **word-sense disambiguation** (WSD).  \n",
    "   - Hard to capture **nuances of context** in large datasets.\n",
    "\n",
    "### 3Ô∏è‚É£ Colloquialisms, Slang & Idioms  \n",
    "   - **Example**: *\"Piece of cake\" (very easy), \"Pulling your leg\" (joking).*  \n",
    "   - Hard for models to **grasp cultural nuances** & informal speech.\n",
    "\n",
    "### 4Ô∏è‚É£ Synonyms & Polysemy  \n",
    "   - **Example**: *\"Buy\" vs. \"Purchase\" (same meaning, different words).*  \n",
    "   - One word can have multiple meanings (**polysemy**).  \n",
    "   - Word embeddings sometimes **struggle** with exact substitutions.\n",
    "\n",
    "### 5Ô∏è‚É£ Irony, Sarcasm & Tonal Differences  \n",
    "   - *\"Oh great! Another meeting. Just what I needed today!\"* üòè  \n",
    "   - **Challenges**:  \n",
    "     - Sentiment analysis models often fail to **detect sarcasm**.  \n",
    "     - Requires **context-awareness & tonal understanding**.\n",
    "\n",
    "### 6Ô∏è‚É£ Spelling Errors & Typos  \n",
    "   - **Example**: *\"teh\" ‚Üí \"the\", \"recieve\" ‚Üí \"receive\"*.  \n",
    "   - Traditional NLP models fail with **misspelled words** unless trained with noisy datasets.  \n",
    "   - **Challenge**: Generalizing across different spelling variations.\n",
    "\n",
    "### 7Ô∏è‚É£ Creativity & Generative Text  \n",
    "   - **Example**: Poems, scripts, novels, humor, and artistic writing.  \n",
    "   - **Challenges**:  \n",
    "     - Capturing **creativity & emotional depth**.  \n",
    "     - Generating **coherent & engaging** content.\n",
    "\n",
    "### 8Ô∏è‚É£ Diversity & Multilingual Challenges  \n",
    "   - **Different grammatical structures** across languages.  \n",
    "   - **Code-switching** (mixing languages in a sentence).  \n",
    "   - **Low-resource languages** (many languages lack sufficient training data).  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Conclusion**  \n",
    "NLP faces major challenges in **understanding human language as humans do**. Overcoming these requires:  \n",
    "‚úÖ **Context-aware models (Transformers like BERT, GPT, T5)**  \n",
    "‚úÖ **Larger, diverse, and well-annotated datasets**  \n",
    "‚úÖ **Improved handling of sarcasm, idioms, and multilingual data**  \n",
    "‚úÖ **Advancements in multimodal learning (combining text, speech, and images).**  \n",
    "\n",
    "üîç *The future of NLP lies in creating models that understand, reason, and generate human-like responses more effectively!* üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb66553-e22a-4ab2-ab3a-c54bf8c1a54c",
   "metadata": {},
   "source": [
    "# üîç What is NLP Pipeline?\n",
    "\n",
    "NLP (Natural Language Processing) is a set of steps followed to build an **end-to-end NLP software**.  \n",
    "The NLP pipeline consists of the following major steps:\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **1. Data Acquisition**\n",
    "   - Collecting raw text data from various sources (web, social media, documents, etc.).\n",
    "\n",
    "### üìù **2. Text Preparation**\n",
    "   - **Text Cleanup** üßπ: Removing unwanted characters, symbols, and formatting issues.\n",
    "   - **Basic Preprocessing** üî§: Tokenization, lowercasing, stop-word removal, stemming, and lemmatization.\n",
    "   - **Advanced Preprocessing** üõ†Ô∏è: Named Entity Recognition (NER), POS tagging, dependency parsing.\n",
    "\n",
    "### ‚öôÔ∏è **3. Feature Engineering**\n",
    "   - Converting text into numerical representations (TF-IDF, word embeddings, BERT embeddings, etc.).\n",
    "   - Handling missing values and preparing data for modeling.\n",
    "\n",
    "### üèóÔ∏è **4. Modelling**\n",
    "   - **Model Building** ü§ñ: Applying ML/DL models like Na√Øve Bayes, LSTMs, Transformers (BERT, GPT).\n",
    "   - **Evaluation** üìä: Measuring performance using metrics like accuracy, F1-score, BLEU, and perplexity.\n",
    "\n",
    "### üöÄ **5. Deployment**\n",
    "   - **Deployment** üì°: Integrating NLP models into applications (APIs, chatbots, search engines).\n",
    "   - **Monitoring** üìà: Tracking model performance in real-world usage.\n",
    "   - **Model Updates** üîÑ: Retraining the model with new data to improve accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Conclusion**\n",
    "The **NLP pipeline** ensures smooth processing of natural language, from raw data collection to real-world application deployment.  \n",
    "To build effective NLP solutions, **each stage must be optimized carefully** to handle challenges like ambiguity, context, and efficiency. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d0d4d-974a-431a-8934-8481a15b4986",
   "metadata": {},
   "source": [
    "# üìä Data Acquisition\n",
    "\n",
    "Data acquisition is the **first step** in an NLP pipeline. It involves collecting and preparing text data for further processing.\n",
    "\n",
    "### **1Ô∏è‚É£ Data Sources**\n",
    "   - **Available Data**\n",
    "     - ‚úÖ **Text data** (pre-existing datasets)\n",
    "     - ‚úÖ **Databases** (structured storage)\n",
    "     - üîπ Requires **Data Engineering** for preprocessing and cleaning.\n",
    "   \n",
    "   - **Other Sources**\n",
    "     - May include **manual collection**, web scraping, or third-party APIs, images, pdf, audio etc.\n",
    "   \n",
    "   - **No Data Available**\n",
    "     - In case of missing data, NLP practitioners must **generate synthetic data**.\n",
    "\n",
    "### **2Ô∏è‚É£ Handling Less Data**\n",
    "   - When data is insufficient, techniques such as **Data Augmentation** help expand the dataset.\n",
    "\n",
    "### **3Ô∏è‚É£ Data Augmentation Techniques**\n",
    "   - **Synonym Replacement** (synonym-based substitution)\n",
    "   - **Bigram Flip** (rearranging word pairs)\n",
    "   - **Back Translation** (translating to another language and back)\n",
    "   - **Adding Noise** (introducing small perturbations to simulate variability)\n",
    "   - **FODS** (unspecified but possibly referring to a specific augmentation strategy)\n",
    "\n",
    "---\n",
    "\n",
    "### üî• **Conclusion**\n",
    "- **Data Acquisition** is crucial for NLP model performance.\n",
    "- **Quality & Quantity** of data significantly impact accuracy.\n",
    "- **Data Augmentation** techniques can help overcome data scarcity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e9f0d-9eab-462b-a70c-5a565238b5c0",
   "metadata": {},
   "source": [
    "# AI Data Preparation\n",
    "\n",
    "## 2. Text Preparation\n",
    "   - **Cleaning**\n",
    "     - HTML tag cleaning\n",
    "     - Emoji removal\n",
    "     - Spelling check\n",
    "\n",
    "   - **Basic Preprocessing**\n",
    "     - **Tokenization**\n",
    "       - Sentence-level tokenization\n",
    "       - Word-level tokenization\n",
    "\n",
    "   - **Advanced Preprocessing**\n",
    "     - POS tagging\n",
    "     - Parsing\n",
    "     - Coreference resolution\n",
    "\n",
    "   - **Optimal Preprocessing**\n",
    "     - Stop word removal\n",
    "     - Stemming\n",
    "     - Removing digits & punctuation\n",
    "     - Lowercasing\n",
    "     - Language detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f045f-6f9e-4f38-9f1e-b6a1cb99ebb0",
   "metadata": {},
   "source": [
    "# AI Text Representation Techniques\n",
    "\n",
    "## 1. Bag of Words (BoW)\n",
    "   - **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "   - **One-Hot Encoding (OHE)**\n",
    "\n",
    "## 2. Word Embeddings\n",
    "   - **Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf93466-cac4-4f6c-a75b-1a5aa8bcc92c",
   "metadata": {},
   "source": [
    "# **Feature Engineering in NLP: Machine Learning vs Deep Learning**\n",
    "\n",
    "Feature engineering plays a crucial role in Natural Language Processing (NLP), where raw text is transformed into numerical representations for models to process. In traditional **Machine Learning (ML)**, feature engineering is often manual, whereas **Deep Learning (DL)** can automatically learn feature representations.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Machine Learning: Advantages & Disadvantages in Feature Engineering**\n",
    "\n",
    "### ‚úÖ **Advantages:**\n",
    "- **Interpretability:** Manually engineered features (e.g., TF-IDF, n-grams, POS tags) help in understanding model decisions.\n",
    "- **Less Computationally Expensive:** Compared to deep learning, ML models require fewer resources.\n",
    "- **Works Well with Small Data:** ML-based approaches do not require large-scale data for training.\n",
    "- **Domain-Specific Feature Customization:** Engineers can manually design features relevant to specific tasks.\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "- **Manual Effort Required:** Significant domain expertise is needed to craft useful features.\n",
    "- **Limited Feature Extraction:** ML struggles to capture deep contextual information from text.\n",
    "- **Scalability Issues:** As the dataset grows, traditional ML models may struggle with complex relationships.\n",
    "- **Feature Dependency:** Performance highly depends on the quality of handcrafted features.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Deep Learning: Advantages & Disadvantages in Feature Engineering**\n",
    "\n",
    "### ‚úÖ **Advantages:**\n",
    "- **Automatic Feature Extraction:** Deep Learning models (e.g., CNNs, RNNs, Transformers) learn hierarchical representations automatically.\n",
    "- **Better Context Understanding:** Models like BERT and GPT capture long-range dependencies and contextual meanings effectively.\n",
    "- **Scalability:** Works well with large datasets and complex problems.\n",
    "- **Higher Accuracy:** Achieves superior performance on many NLP tasks like sentiment analysis, translation, and summarization.\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "- **Computational Cost:** Requires high-end GPUs/TPUs and extensive training time.\n",
    "- **Data Hungry:** Deep Learning models perform best when trained on massive datasets.\n",
    "- **Black Box Nature:** Difficult to interpret why a deep learning model makes specific predictions.\n",
    "- **Overfitting Risk:** In small datasets, deep models may memorize patterns instead of generalizing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion: Which One to Use?**\n",
    "- If **data is limited** and **interpretability is important**, **Machine Learning** is preferable.\n",
    "- If **large-scale data** and **higher accuracy** are required, **Deep Learning** is the better choice.\n",
    "\n",
    "For many modern NLP applications, **deep learning is the dominant choice**, but **machine learning still remains useful** in certain resource-constrained scenarios.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd844a-9abc-4023-a9be-a302e19670b1",
   "metadata": {},
   "source": [
    "# **Modelling in NLP**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Key Components**\n",
    "- **Modelling**\n",
    "- **Evaluation** (using **BERT**)\n",
    "\n",
    "## **2. Spam Classification Approaches**\n",
    "- **Heuristic-based Methods**\n",
    "- **Machine Learning (ML) Approaches**\n",
    "  - Rule-based models\n",
    "  - Traditional classifiers (e.g., SVM, Decision Trees)\n",
    "- **Deep Learning (DL) Approaches**\n",
    "  - Neural Networks\n",
    "  - Transformer models (BERT, LSTMs, CNNs)\n",
    "- **Cloud API-Based Approaches**\n",
    "\n",
    "## **3. Transfer Learning**\n",
    "- Leveraging pre-trained models for spam classification.\n",
    "\n",
    "## **4. Evaluation Metrics**\n",
    "- **# of positive reviews (+ve)**\n",
    "- **# of negative reviews (-ve)**\n",
    "- **# of spammed reviews**\n",
    "\n",
    "## **5. Influencing Factors**\n",
    "- **Amount of Data**: More data generally improves model performance.\n",
    "- **Nature of Problem**: The complexity of the problem determines the best modelling approach.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Components of Modelling**\n",
    "- **Evaluation** (uses BERT for NLP tasks)\n",
    "- **Modelling Approaches:**\n",
    "  - **Heuristic-Based Methods**\n",
    "  - **Machine Learning (ML) Approaches**\n",
    "    - Traditional algorithms (e.g., SVM, Naive Bayes)\n",
    "  - **Deep Learning (DL) Approaches**\n",
    "    - Neural networks, Transformers (e.g., BERT)\n",
    "  - **Cloud APIs**\n",
    "    - Pre-trained models for NLP tasks\n",
    "\n",
    "## **2. Spam Classifier Workflow**\n",
    "- Different approaches to spam classification:\n",
    "  - **Heuristic Rules**\n",
    "  - **Machine Learning Models**\n",
    "  - **Deep Learning Models**\n",
    "  - **Cloud API Services**\n",
    "- Classification Results:\n",
    "  - **# of Positive Cases (# +ve)**\n",
    "  - **# of Negative Cases (# -ve)**\n",
    "  - **# of Spam Messages (Boom Spammed)**\n",
    "\n",
    "## **3. Transfer Learning in NLP**\n",
    "- Utilized to enhance model performance\n",
    "- Allows adaptation of pre-trained models (e.g., BERT, GPT)\n",
    "\n",
    "## **4. Factors Affecting Model Choice**\n",
    "- **Amount of Data:** More data favors deep learning.\n",
    "- **Nature of the Problem:** Complexity determines the approach (ML, DL, or heuristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904befb-39e6-4b68-9dae-92362c0069d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
