{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04289a8e-097d-4582-83f7-30d230863a7e",
   "metadata": {},
   "source": [
    "### Common Stemming Algorithms in Natural Language Processing\"\n",
    "\n",
    "#### Description:\n",
    "- Stemming algorithms are used in natural language processing to reduce words to their base or root forms by removing common morphological suffixes. Here are some popular stemming algorithms:\n",
    "\n",
    "- **Porter Stemmer**: A widely used stemming algorithm that effectively removes common English word suffixes.\n",
    "- Lovins Stemmer\n",
    "- Dawson Stemmer\n",
    "- Krovetz Stemmer\n",
    "- Xerox Stemmer\n",
    "- N-Gram Stemmer\n",
    "- Snowball Stemmer (an improved version of the Porter Stemmer)\n",
    "- Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa42b56-1c19-49c0-85db-cf400f75fa4c",
   "metadata": {},
   "source": [
    "## Workflow \n",
    "- Tokenization must be done first to prepare the text for lemmatization.\n",
    "- **Tokenization â†’ Stemming/Lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a50ea94-ad56-496c-a152-ed58e57d8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61224504-5584-45da-9339-1e133742e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')  # Lexical database for English\n",
    "nltk.download('maxent_ne_chunker')  # NER models for entity recognition\n",
    "nltk.download('words')  # Corpus of English words\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003fd98a-59ea-42e2-b2df-804ca8576445",
   "metadata": {},
   "source": [
    "### WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a568de43-c2fe-4cf7-b8da-3aeb9a8c9f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['happy'], ['felicitous', 'happy'], ['glad', 'happy'], ['happy', 'well-chosen']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Find synonyms of a word\n",
    "synonyms = wordnet.synsets(\"happy\")\n",
    "print([syn.lemma_names() for syn in synonyms])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72a732-b0df-4094-9971-9c636ebb3fc9",
   "metadata": {},
   "source": [
    "### Download importance pakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac1fb68-9585-418e-b558-3a931ebdb934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')  # Required for NE chunking\n",
    "nltk.download('averaged_perceptron_tagger')  # Ensure POS tagger is downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44aba7f-23dd-4d22-98af-8f20d29355a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find('chunkers/maxent_ne_chunker')\n",
    "nltk.data.find('corpora/words')\n",
    "nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13a99da-4667-4c28-9b85-7d848aee94eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\User/nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873e32d-531d-4433-bdfe-a8165130c5a7",
   "metadata": {},
   "source": [
    "## For Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d097fac7-6479-418d-a083-cf0b327f09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word   | Stemmed Word   \n",
      "---------------------------------\n",
      "change          | chang          \n",
      "changing        | chang          \n",
      "changes         | chang          \n",
      "changed         | chang          \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 1: Define a list of words\n",
    "words = ['change', 'changing', 'changes', 'changed']\n",
    "\n",
    "# Step 2: Initialize PorterStemmer\n",
    "p = PorterStemmer()\n",
    "\n",
    "# Step 3: Stem each word in the list\n",
    "print(f\"{'Original Word':<15} | {'Stemmed Word':<15}\")\n",
    "print(\"-\" * 33)\n",
    "for w in words:\n",
    "    print(f\"{w:<15} | {p.stem(w):<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a4bf9-9d58-4769-8499-908566cfa01f",
   "metadata": {},
   "source": [
    "## For sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe506500-9848-4f73-b7bf-d2786332faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word   | Stemmed Word   \n",
      "---------------------------------\n",
      "The             | the            \n",
      "constant        | constant       \n",
      "flux            | flux           \n",
      "of              | of             \n",
      "life            | life           \n",
      "necessitates    | necessit       \n",
      "embracing       | embrac         \n",
      "change          | chang          \n",
      ",               | ,              \n",
      "whether         | whether        \n",
      "it              | it             \n",
      "'s              | 's             \n",
      "adapting        | adapt          \n",
      "to              | to             \n",
      "the             | the            \n",
      "changes         | chang          \n",
      "around          | around         \n",
      "us              | us             \n",
      "or              | or             \n",
      "actively        | activ          \n",
      "changing        | chang          \n",
      "ourselves       | ourselv        \n",
      "to              | to             \n",
      "meet            | meet           \n",
      "new             | new            \n",
      "challenges      | challeng       \n",
      ".               | .              \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"The constant flux of life necessitates embracing change, whether it's adapting to the changes around us or actively changing ourselves to meet new challenges.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each token and print in a structured format\n",
    "print(f\"{'Original Word':<15} | {'Stemmed Word':<15}\")\n",
    "print(\"-\" * 33)\n",
    "for token in tokens:\n",
    "    print(f\"{token:<15} | {stemmer.stem(token):<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717428e-6aa3-40d1-a39d-c8ed6afb0a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d993d15d-4df0-41fc-9322-bf2a14d41512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['run', 'runner', 'run', 'easili', 'faster']\n",
      "Lancaster Stemmer: ['run', 'run', 'run', 'easy', 'fast']\n",
      "Snowball Stemmer: ['run', 'runner', 'run', 'easili', 'faster']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"running runner runs easily faster\"\n",
    "\n",
    "# Tokenize sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# 1. Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "porter_stems = [porter.stem(word) for word in words]\n",
    "print(\"Porter Stemmer:\", porter_stems)\n",
    "\n",
    "# 2. Lancaster Stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster_stems = [lancaster.stem(word) for word in words]\n",
    "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
    "\n",
    "# 3. Snowball Stemmer\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "snowball_stems = [snowball.stem(word) for word in words]\n",
    "print(\"Snowball Stemmer:\", snowball_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0956c03-28cb-4a7c-8d3f-954f8e742c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"The constant flux of life necessitates embracing change, whether it's adapting to the changes around us or actively changing ourselves to meet new challenges.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad95f70-5d45-4865-ae2d-347e3b7da579",
   "metadata": {},
   "source": [
    "### Tokenization best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b479d5a5-7386-46ec-a675-d0168a70c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word | Stemmed Word\n",
      "----------------------------\n",
      "The             | the\n",
      "constant        | constant\n",
      "flux            | flux\n",
      "of              | of\n",
      "life            | life\n",
      "necessitates    | necessit\n",
      "embracing       | embrac\n",
      "change          | chang\n",
      ",               | ,\n",
      "whether         | whether\n",
      "it              | it\n",
      "'s              | 's\n",
      "adapting        | adapt\n",
      "to              | to\n",
      "the             | the\n",
      "changes         | chang\n",
      "around          | around\n",
      "us              | us\n",
      "or              | or\n",
      "actively        | activ\n",
      "changing        | chang\n",
      "ourselves       | ourselv\n",
      "to              | to\n",
      "meet            | meet\n",
      "new             | new\n",
      "challenges      | challeng\n",
      ".               | .\n"
     ]
    }
   ],
   "source": [
    "# Apply stemming to each token\n",
    "print(\"Original Word | Stemmed Word\")\n",
    "print(\"----------------------------\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:15} | {stemmer.stem(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82877c20-c887-47a2-93e3-3f2748564a2a",
   "metadata": {},
   "source": [
    "### Using split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed6dd456-0056-4002-9519-77415d735af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack',\n",
       " 'Obama',\n",
       " 'was',\n",
       " 'the',\n",
       " '44th',\n",
       " 'President',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "701abfa3-05a8-4257-8ceb-df3e02be0b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embracing',\n",
       " 'change,',\n",
       " 'whether',\n",
       " \"it's\",\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c2b05-99dc-434b-9693-f4bd65e1b796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
